{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talking Healthcare Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5057.14s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /home/siyamak/Documents/MLHub/venv/bin/pip: cannot execute: required file not found\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load intents from JSON file\n",
    "intents = json.loads(open(\"intents.json\").read())\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "\n",
    "ignore_letters = [\"?\", \"!\", \".\", \",\"]\n",
    "\n",
    "# Process intents\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent[\"tag\"]))\n",
    "\n",
    "        if intent[\"tag\"] not in classes:\n",
    "            classes.append(intent[\"tag\"])\n",
    "\n",
    "#\n",
    "# Save words and classes to pickle files\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = []\n",
    "template = [0] * len(classes)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(template)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    dataset.append([bag, output_row])\n",
    "\n",
    "# Shuffle dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Check lengths before converting to numpy array\n",
    "for entry in dataset:\n",
    "    bag, output_row = entry\n",
    "    if len(bag) != len(words) or len(output_row) != len(classes):\n",
    "        print(f\"Length mismatch: Bag length: {len(bag)}, Output row length: {len(output_row)}\")\n",
    "\n",
    "# Convert dataset to numpy array\n",
    "try:\n",
    "    dataset = np.array(dataset, dtype=object)  # Use dtype=object to handle variable length arrays\n",
    "except ValueError as e:\n",
    "    print(\"Error converting dataset to numpy array:\", e)\n",
    "\n",
    "# Split dataset into training data\n",
    "train_x = np.array([item[0] for item in dataset])  # Extract bags\n",
    "train_y = np.array([item[1] for item in dataset])  # Extract output rows\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "hist = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"chatbot_model.h5\")\n",
    "print(\"Done!\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pexpect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip install speechrecognition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLHub/venv/lib/python3.12/site-packages/ipykernel/zmqshell.py:657\u001b[0m, in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/MLHub/venv/lib/python3.12/site-packages/IPython/utils/_process_posix.py:125\u001b[0m, in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pexpect'"
     ]
    }
   ],
   "source": [
    "! pip install speechrecognition  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "# بارگذاری منابع\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents = json.loads(open(\"intents.json\").read())\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "model = load_model('chatbot_model.h5')\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == w:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = model.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    result = ''\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def calling_the_bot(txt):\n",
    "    predict = predict_class(txt)\n",
    "    res = get_response(predict, intents)\n",
    "    engine.say(\"Found it. From our Database we found that \" + res)\n",
    "    engine.runAndWait()\n",
    "    print(\"Your Symptom was: \", txt)\n",
    "    print(\"Result found in our Database: \", res)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Bot is Running\")\n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    engine = pyttsx3.init()\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', 175)\n",
    "    volume = engine.getProperty('volume')\n",
    "    engine.setProperty('volume', 1.0)\n",
    "    voices = engine.getProperty('voices')\n",
    "\n",
    "    # خوشامدگویی\n",
    "    engine.say(\"Hello user, I am Bagley, your personal Talking Healthcare Chatbot.\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "    engine.say(\"IF YOU WANT TO CONTINUE WITH MALE VOICE PLEASE SAY MALE. OTHERWISE SAY FEMALE.\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "    # انتخاب صدای مرد یا زن\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=0.2)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    audio = recognizer.recognize_google(audio)\n",
    "\n",
    "    if audio.lower() == \"female\":\n",
    "        engine.setProperty('voice', voices[1].id)\n",
    "        print(\"You have chosen to continue with Female Voice\")\n",
    "    else:\n",
    "        engine.setProperty('voice', voices[0].id)\n",
    "        print(\"You have chosen to continue with Male Voice\")\n",
    "\n",
    "    while True:\n",
    "        with mic as symptom:\n",
    "            print(\"Say Your Symptoms. The Bot is Listening\")\n",
    "            engine.say(\"You may tell me your symptoms now. I am listening\")\n",
    "            engine.runAndWait()\n",
    "            try:\n",
    "                recognizer.adjust_for_ambient_noise(symptom, duration=0.2)\n",
    "                symp = recognizer.listen(symptom)\n",
    "                text = recognizer.recognize_google(symp)\n",
    "                engine.say(\"You said {}\".format(text))\n",
    "                engine.runAndWait()\n",
    "\n",
    "                engine.say(\"Scanning our database for your symptom. Please wait.\")\n",
    "                engine.runAndWait()\n",
    "                time.sleep(1)\n",
    "\n",
    "                # فراخوانی تابع\n",
    "                calling_the_bot(text)\n",
    "            except sr.UnknownValueError:\n",
    "                engine.say(\"Sorry, Either your symptom is unclear to me or it is not present in our database. Please Try Again.\")\n",
    "                engine.runAndWait()\n",
    "                print(\"Sorry, Either your symptom is unclear to me or it is not present in our database. Please Try Again.\")\n",
    "            finally:\n",
    "                engine.say(\"If you want to continue please say True otherwise say False.\")\n",
    "                engine.runAndWait()\n",
    "\n",
    "        with mic as ans:\n",
    "            recognizer.adjust_for_ambient_noise(ans, duration=0.2)\n",
    "            voice = recognizer.listen(ans)\n",
    "            final = recognizer.recognize_google(voice)\n",
    "\n",
    "        if final.lower() in ['no', 'please exit']:\n",
    "            engine.say(\"Thank You. Shutting Down now.\")\n",
    "            engine.runAndWait()\n",
    "            print(\"Bot has been stopped by the user\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
